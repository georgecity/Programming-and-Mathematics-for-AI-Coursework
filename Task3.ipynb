{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Pytorch Neural Networks Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import numpy as np \n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_digits = torchvision.datasets.MNIST(root='./', train=True, download=False, transform=tr )\n",
    "test_digits = torchvision.datasets.MNIST(root='./', train=False, download=False, transform=tr )\n",
    "\n",
    "train, val = random_split(train_digits, [55000, 5000])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_digits, batch_size=64, shuffle=True)"
   ]
  },
  {
   "source": [
    "# Pytorch Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Nnet, self).__init__()\n",
    "        self.lin1 = nn.Linear(28*28, 128)\n",
    "        self.lin2 = nn.Linear(128, 64)\n",
    "        self.lin3 = nn.Linear(64, 64)\n",
    "        self.lin4 = nn.Linear(64, 32)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, images):\n",
    "        x = images.view(-1, 28*28)\n",
    "        x = self.relu(self.lin1(x))\n",
    "        x = self.sigmoid(self.lin2(x))\n",
    "        x = self.relu(self.lin3(x))\n",
    "        x = self.softmax(self.lin4(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Nnet()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "params = model.parameters()\n",
    "opt = optim.Adam(params, lr=0.001)\n",
    "epochs = 1250\n",
    "iters = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".75\n",
      "Epoch 892, training loss: 2.80,training accuracy: 0.72\n",
      "Epoch 893, training loss: 2.77,training accuracy: 0.77\n",
      "Epoch 894, training loss: 2.76,training accuracy: 0.80\n",
      "Epoch 895, training loss: 2.71,training accuracy: 0.83\n",
      "Epoch 896, training loss: 2.74,training accuracy: 0.80\n",
      "Epoch 897, training loss: 2.82,training accuracy: 0.70\n",
      "Epoch 898, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 899, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 900, training loss: 2.75,training accuracy: 0.77\n",
      "Epoch 901, training loss: 2.73,training accuracy: 0.83\n",
      "Epoch 902, training loss: 2.70,training accuracy: 0.81\n",
      "Epoch 903, training loss: 2.71,training accuracy: 0.83\n",
      "Epoch 904, training loss: 2.73,training accuracy: 0.80\n",
      "Epoch 905, training loss: 2.73,training accuracy: 0.80\n",
      "Epoch 906, training loss: 2.71,training accuracy: 0.83\n",
      "Epoch 907, training loss: 2.77,training accuracy: 0.77\n",
      "Epoch 908, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 909, training loss: 2.73,training accuracy: 0.81\n",
      "Epoch 910, training loss: 2.76,training accuracy: 0.77\n",
      "Epoch 911, training loss: 2.82,training accuracy: 0.72\n",
      "Epoch 912, training loss: 2.70,training accuracy: 0.84\n",
      "Epoch 913, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 914, training loss: 2.76,training accuracy: 0.77\n",
      "Epoch 915, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 916, training loss: 2.76,training accuracy: 0.77\n",
      "Epoch 917, training loss: 2.74,training accuracy: 0.80\n",
      "Epoch 918, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 919, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 920, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 921, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 922, training loss: 2.70,training accuracy: 0.84\n",
      "Epoch 923, training loss: 2.66,training accuracy: 0.86\n",
      "Epoch 924, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 925, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 926, training loss: 2.76,training accuracy: 0.77\n",
      "Epoch 927, training loss: 2.73,training accuracy: 0.80\n",
      "Epoch 928, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 929, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 930, training loss: 2.66,training accuracy: 0.91\n",
      "Epoch 931, training loss: 2.76,training accuracy: 0.75\n",
      "Epoch 932, training loss: 2.71,training accuracy: 0.83\n",
      "Epoch 933, training loss: 2.62,training accuracy: 0.91\n",
      "Epoch 934, training loss: 2.78,training accuracy: 0.75\n",
      "Epoch 935, training loss: 2.73,training accuracy: 0.80\n",
      "Epoch 936, training loss: 2.76,training accuracy: 0.77\n",
      "Epoch 937, training loss: 2.72,training accuracy: 0.78\n",
      "Epoch 938, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 939, training loss: 2.64,training accuracy: 0.88\n",
      "Epoch 940, training loss: 2.76,training accuracy: 0.77\n",
      "Epoch 941, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 942, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 943, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 944, training loss: 2.70,training accuracy: 0.86\n",
      "Epoch 945, training loss: 2.75,training accuracy: 0.78\n",
      "Epoch 946, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 947, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 948, training loss: 2.62,training accuracy: 0.91\n",
      "Epoch 949, training loss: 2.71,training accuracy: 0.83\n",
      "Epoch 950, training loss: 2.79,training accuracy: 0.72\n",
      "Epoch 951, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 952, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 953, training loss: 2.65,training accuracy: 0.89\n",
      "Epoch 954, training loss: 2.74,training accuracy: 0.80\n",
      "Epoch 955, training loss: 2.75,training accuracy: 0.78\n",
      "Epoch 956, training loss: 2.63,training accuracy: 0.91\n",
      "Epoch 957, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 958, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 959, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 960, training loss: 2.79,training accuracy: 0.72\n",
      "Epoch 961, training loss: 2.72,training accuracy: 0.78\n",
      "Epoch 962, training loss: 2.73,training accuracy: 0.80\n",
      "Epoch 963, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 964, training loss: 2.65,training accuracy: 0.89\n",
      "Epoch 965, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 966, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 967, training loss: 2.77,training accuracy: 0.75\n",
      "Epoch 968, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 969, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 970, training loss: 2.74,training accuracy: 0.77\n",
      "Epoch 971, training loss: 2.67,training accuracy: 0.84\n",
      "Epoch 972, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 973, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 974, training loss: 2.80,training accuracy: 0.72\n",
      "Epoch 975, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 976, training loss: 2.73,training accuracy: 0.81\n",
      "Epoch 977, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 978, training loss: 2.66,training accuracy: 0.86\n",
      "Epoch 979, training loss: 2.74,training accuracy: 0.78\n",
      "Epoch 980, training loss: 2.76,training accuracy: 0.77\n",
      "Epoch 981, training loss: 2.74,training accuracy: 0.78\n",
      "Epoch 982, training loss: 2.64,training accuracy: 0.88\n",
      "Epoch 983, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 984, training loss: 2.66,training accuracy: 0.86\n",
      "Epoch 985, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 986, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 987, training loss: 2.79,training accuracy: 0.72\n",
      "Epoch 988, training loss: 2.62,training accuracy: 0.91\n",
      "Epoch 989, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 990, training loss: 2.76,training accuracy: 0.77\n",
      "Epoch 991, training loss: 2.63,training accuracy: 0.91\n",
      "Epoch 992, training loss: 2.72,training accuracy: 0.78\n",
      "Epoch 993, training loss: 2.78,training accuracy: 0.73\n",
      "Epoch 994, training loss: 2.74,training accuracy: 0.78\n",
      "Epoch 995, training loss: 2.72,training accuracy: 0.81\n",
      "Epoch 996, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 997, training loss: 2.73,training accuracy: 0.80\n",
      "Epoch 998, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 999, training loss: 2.71,training accuracy: 0.83\n",
      "Epoch 1000, training loss: 2.73,training accuracy: 0.80\n",
      "Epoch 1001, training loss: 2.63,training accuracy: 0.88\n",
      "Epoch 1002, training loss: 2.67,training accuracy: 0.83\n",
      "Epoch 1003, training loss: 2.72,training accuracy: 0.81\n",
      "Epoch 1004, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 1005, training loss: 2.78,training accuracy: 0.75\n",
      "Epoch 1006, training loss: 2.79,training accuracy: 0.75\n",
      "Epoch 1007, training loss: 2.67,training accuracy: 0.88\n",
      "Epoch 1008, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 1009, training loss: 2.77,training accuracy: 0.75\n",
      "Epoch 1010, training loss: 2.71,training accuracy: 0.83\n",
      "Epoch 1011, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 1012, training loss: 2.74,training accuracy: 0.80\n",
      "Epoch 1013, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 1014, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 1015, training loss: 2.73,training accuracy: 0.81\n",
      "Epoch 1016, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 1017, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 1018, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 1019, training loss: 2.70,training accuracy: 0.81\n",
      "Epoch 1020, training loss: 2.72,training accuracy: 0.81\n",
      "Epoch 1021, training loss: 2.66,training accuracy: 0.84\n",
      "Epoch 1022, training loss: 2.70,training accuracy: 0.81\n",
      "Epoch 1023, training loss: 2.79,training accuracy: 0.73\n",
      "Epoch 1024, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 1025, training loss: 2.76,training accuracy: 0.77\n",
      "Epoch 1026, training loss: 2.72,training accuracy: 0.83\n",
      "Epoch 1027, training loss: 2.74,training accuracy: 0.81\n",
      "Epoch 1028, training loss: 2.63,training accuracy: 0.89\n",
      "Epoch 1029, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 1030, training loss: 2.66,training accuracy: 0.84\n",
      "Epoch 1031, training loss: 2.64,training accuracy: 0.89\n",
      "Epoch 1032, training loss: 2.78,training accuracy: 0.75\n",
      "Epoch 1033, training loss: 2.79,training accuracy: 0.73\n",
      "Epoch 1034, training loss: 2.65,training accuracy: 0.86\n",
      "Epoch 1035, training loss: 2.75,training accuracy: 0.77\n",
      "Epoch 1036, training loss: 2.79,training accuracy: 0.73\n",
      "Epoch 1037, training loss: 2.65,training accuracy: 0.89\n",
      "Epoch 1038, training loss: 2.64,training accuracy: 0.89\n",
      "Epoch 1039, training loss: 2.62,training accuracy: 0.89\n",
      "Epoch 1040, training loss: 2.66,training accuracy: 0.86\n",
      "Epoch 1041, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 1042, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 1043, training loss: 2.68,training accuracy: 0.83\n",
      "Epoch 1044, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 1045, training loss: 2.67,training accuracy: 0.84\n",
      "Epoch 1046, training loss: 2.85,training accuracy: 0.67\n",
      "Epoch 1047, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 1048, training loss: 2.59,training accuracy: 0.92\n",
      "Epoch 1049, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 1050, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 1051, training loss: 2.70,training accuracy: 0.84\n",
      "Epoch 1052, training loss: 2.71,training accuracy: 0.80\n",
      "Epoch 1053, training loss: 2.72,training accuracy: 0.81\n",
      "Epoch 1054, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 1055, training loss: 2.77,training accuracy: 0.77\n",
      "Epoch 1056, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 1057, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 1058, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 1059, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 1060, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 1061, training loss: 2.72,training accuracy: 0.81\n",
      "Epoch 1062, training loss: 2.76,training accuracy: 0.75\n",
      "Epoch 1063, training loss: 2.74,training accuracy: 0.78\n",
      "Epoch 1064, training loss: 2.62,training accuracy: 0.91\n",
      "Epoch 1065, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 1066, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 1067, training loss: 2.67,training accuracy: 0.88\n",
      "Epoch 1068, training loss: 2.71,training accuracy: 0.83\n",
      "Epoch 1069, training loss: 2.76,training accuracy: 0.73\n",
      "Epoch 1070, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 1071, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 1072, training loss: 2.57,training accuracy: 0.95\n",
      "Epoch 1073, training loss: 2.73,training accuracy: 0.80\n",
      "Epoch 1074, training loss: 2.73,training accuracy: 0.78\n",
      "Epoch 1075, training loss: 2.64,training accuracy: 0.88\n",
      "Epoch 1076, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 1077, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 1078, training loss: 2.70,training accuracy: 0.80\n",
      "Epoch 1079, training loss: 2.66,training accuracy: 0.86\n",
      "Epoch 1080, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 1081, training loss: 2.74,training accuracy: 0.77\n",
      "Epoch 1082, training loss: 2.71,training accuracy: 0.80\n",
      "Epoch 1083, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 1084, training loss: 2.75,training accuracy: 0.80\n",
      "Epoch 1085, training loss: 2.71,training accuracy: 0.80\n",
      "Epoch 1086, training loss: 2.66,training accuracy: 0.86\n",
      "Epoch 1087, training loss: 2.73,training accuracy: 0.80\n",
      "Epoch 1088, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 1089, training loss: 2.75,training accuracy: 0.77\n",
      "Epoch 1090, training loss: 2.69,training accuracy: 0.81\n",
      "Epoch 1091, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 1092, training loss: 2.67,training accuracy: 0.84\n",
      "Epoch 1093, training loss: 2.71,training accuracy: 0.83\n",
      "Epoch 1094, training loss: 2.74,training accuracy: 0.78\n",
      "Epoch 1095, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 1096, training loss: 2.76,training accuracy: 0.78\n",
      "Epoch 1097, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 1098, training loss: 2.70,training accuracy: 0.81\n",
      "Epoch 1099, training loss: 2.64,training accuracy: 0.89\n",
      "Epoch 1100, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 1101, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 1102, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 1103, training loss: 2.74,training accuracy: 0.78\n",
      "Epoch 1104, training loss: 2.73,training accuracy: 0.78\n",
      "Epoch 1105, training loss: 2.78,training accuracy: 0.75\n",
      "Epoch 1106, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 1107, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 1108, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 1109, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 1110, training loss: 2.67,training accuracy: 0.88\n",
      "Epoch 1111, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 1112, training loss: 2.62,training accuracy: 0.91\n",
      "Epoch 1113, training loss: 2.65,training accuracy: 0.89\n",
      "Epoch 1114, training loss: 2.65,training accuracy: 0.86\n",
      "Epoch 1115, training loss: 2.62,training accuracy: 0.91\n",
      "Epoch 1116, training loss: 2.71,training accuracy: 0.83\n",
      "Epoch 1117, training loss: 2.60,training accuracy: 0.92\n",
      "Epoch 1118, training loss: 2.75,training accuracy: 0.77\n",
      "Epoch 1119, training loss: 2.74,training accuracy: 0.78\n",
      "Epoch 1120, training loss: 2.74,training accuracy: 0.78\n",
      "Epoch 1121, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 1122, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 1123, training loss: 2.84,training accuracy: 0.69\n",
      "Epoch 1124, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 1125, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 1126, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 1127, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 1128, training loss: 2.65,training accuracy: 0.89\n",
      "Epoch 1129, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 1130, training loss: 2.62,training accuracy: 0.91\n",
      "Epoch 1131, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 1132, training loss: 2.72,training accuracy: 0.81\n",
      "Epoch 1133, training loss: 2.80,training accuracy: 0.72\n",
      "Epoch 1134, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 1135, training loss: 2.66,training accuracy: 0.86\n",
      "Epoch 1136, training loss: 2.71,training accuracy: 0.80\n",
      "Epoch 1137, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 1138, training loss: 2.75,training accuracy: 0.78\n",
      "Epoch 1139, training loss: 2.68,training accuracy: 0.83\n",
      "Epoch 1140, training loss: 2.66,training accuracy: 0.86\n",
      "Epoch 1141, training loss: 2.64,training accuracy: 0.89\n",
      "Epoch 1142, training loss: 2.63,training accuracy: 0.91\n",
      "Epoch 1143, training loss: 2.63,training accuracy: 0.89\n",
      "Epoch 1144, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 1145, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 1146, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 1147, training loss: 2.64,training accuracy: 0.88\n",
      "Epoch 1148, training loss: 2.61,training accuracy: 0.92\n",
      "Epoch 1149, training loss: 2.72,training accuracy: 0.78\n",
      "Epoch 1150, training loss: 2.75,training accuracy: 0.75\n",
      "Epoch 1151, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 1152, training loss: 2.77,training accuracy: 0.75\n",
      "Epoch 1153, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 1154, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 1155, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 1156, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 1157, training loss: 2.77,training accuracy: 0.75\n",
      "Epoch 1158, training loss: 2.72,training accuracy: 0.81\n",
      "Epoch 1159, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 1160, training loss: 2.67,training accuracy: 0.84\n",
      "Epoch 1161, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 1162, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 1163, training loss: 2.66,training accuracy: 0.86\n",
      "Epoch 1164, training loss: 2.63,training accuracy: 0.89\n",
      "Epoch 1165, training loss: 2.61,training accuracy: 0.92\n",
      "Epoch 1166, training loss: 2.72,training accuracy: 0.78\n",
      "Epoch 1167, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 1168, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 1169, training loss: 2.72,training accuracy: 0.81\n",
      "Epoch 1170, training loss: 2.64,training accuracy: 0.88\n",
      "Epoch 1171, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 1172, training loss: 2.72,training accuracy: 0.81\n",
      "Epoch 1173, training loss: 2.59,training accuracy: 0.92\n",
      "Epoch 1174, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 1175, training loss: 2.81,training accuracy: 0.72\n",
      "Epoch 1176, training loss: 2.80,training accuracy: 0.72\n",
      "Epoch 1177, training loss: 2.70,training accuracy: 0.81\n",
      "Epoch 1178, training loss: 2.61,training accuracy: 0.92\n",
      "Epoch 1179, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 1180, training loss: 2.67,training accuracy: 0.84\n",
      "Epoch 1181, training loss: 2.75,training accuracy: 0.77\n",
      "Epoch 1182, training loss: 2.66,training accuracy: 0.86\n",
      "Epoch 1183, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 1184, training loss: 2.62,training accuracy: 0.91\n",
      "Epoch 1185, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 1186, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 1187, training loss: 2.64,training accuracy: 0.91\n",
      "Epoch 1188, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 1189, training loss: 2.69,training accuracy: 0.86\n",
      "Epoch 1190, training loss: 2.68,training accuracy: 0.83\n",
      "Epoch 1191, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 1192, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 1193, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 1194, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 1195, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 1196, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 1197, training loss: 2.62,training accuracy: 0.91\n",
      "Epoch 1198, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 1199, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 1200, training loss: 2.76,training accuracy: 0.75\n",
      "Epoch 1201, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 1202, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 1203, training loss: 2.66,training accuracy: 0.89\n",
      "Epoch 1204, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 1205, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 1206, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 1207, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 1208, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 1209, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 1210, training loss: 2.64,training accuracy: 0.91\n",
      "Epoch 1211, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 1212, training loss: 2.73,training accuracy: 0.81\n",
      "Epoch 1213, training loss: 2.66,training accuracy: 0.86\n",
      "Epoch 1214, training loss: 2.62,training accuracy: 0.91\n",
      "Epoch 1215, training loss: 2.68,training accuracy: 0.83\n",
      "Epoch 1216, training loss: 2.63,training accuracy: 0.89\n",
      "Epoch 1217, training loss: 2.64,training accuracy: 0.89\n",
      "Epoch 1218, training loss: 2.76,training accuracy: 0.77\n",
      "Epoch 1219, training loss: 2.74,training accuracy: 0.77\n",
      "Epoch 1220, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 1221, training loss: 2.74,training accuracy: 0.78\n",
      "Epoch 1222, training loss: 2.73,training accuracy: 0.80\n",
      "Epoch 1223, training loss: 2.72,training accuracy: 0.81\n",
      "Epoch 1224, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 1225, training loss: 2.62,training accuracy: 0.89\n",
      "Epoch 1226, training loss: 2.76,training accuracy: 0.77\n",
      "Epoch 1227, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 1228, training loss: 2.68,training accuracy: 0.83\n",
      "Epoch 1229, training loss: 2.73,training accuracy: 0.80\n",
      "Epoch 1230, training loss: 2.74,training accuracy: 0.80\n",
      "Epoch 1231, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 1232, training loss: 2.77,training accuracy: 0.73\n",
      "Epoch 1233, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 1234, training loss: 2.73,training accuracy: 0.80\n",
      "Epoch 1235, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 1236, training loss: 2.64,training accuracy: 0.88\n",
      "Epoch 1237, training loss: 2.64,training accuracy: 0.89\n",
      "Epoch 1238, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 1239, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 1240, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 1241, training loss: 2.58,training accuracy: 0.95\n",
      "Epoch 1242, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 1243, training loss: 2.62,training accuracy: 0.91\n",
      "Epoch 1244, training loss: 2.80,training accuracy: 0.72\n",
      "Epoch 1245, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 1246, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 1247, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 1248, training loss: 2.72,training accuracy: 0.81\n",
      "Epoch 1249, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 1250, training loss: 2.65,training accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    losses = list()\n",
    "    accuracies = list()\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        output = model(images)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss = loss_func(output, labels)\n",
    "        loss.backward() \n",
    "\n",
    "        opt.step() \n",
    "\n",
    "        losses.append(loss.item())\n",
    "        accuracies.append(labels.eq(output.detach().argmax(dim=1)).float().mean())\n",
    "\n",
    "        print(f'Epoch {epoch + 1 }', end= ', ' )\n",
    "        print(f'training loss: {torch.tensor(losses).mean():.2f}', end=',')\n",
    "        print(f'training accuracy: {torch.tensor(accuracies).mean():.2f}')\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_eval(model, test_loader):\n",
    "    predictions = torch.tensor([])\n",
    "    testLabels = torch.tensor([])\n",
    "    for batch in test_loader:\n",
    "        images, label = batch\n",
    "\n",
    "        preds = model(images)\n",
    "        predictions = torch.cat(\n",
    "            (predictions, preds)\n",
    "            ,dim=0\n",
    "        )\n",
    "        testLabels = torch.cat(\n",
    "            (testLabels, label)\n",
    "            ,dim=0\n",
    "        )\n",
    "    predictions = np.array(predictions)\n",
    "    testLabels = np.array(testLabels)\n",
    "    return predictions, testLabels\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions, testLabels = get_eval(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLabels =  to_categorical(testLabels)\n",
    "testLabels = np.argmax(testLabels, axis=1)\n",
    "predictions = np.argmax(predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction:  [3 3 1 9 9 8 1 8 0 2 7 2 0 2 0 6 3 6 6 0]\nActual:  [3 3 1 9 4 4 1 3 0 8 7 2 0 2 0 6 3 6 6 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction: \", predictions[:20])\n",
    "print(\"Actual: \",testLabels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.93      0.98      0.95       980\n           1       0.96      0.99      0.97      1135\n           2       0.90      0.90      0.90      1032\n           3       0.90      0.91      0.90      1010\n           4       0.00      0.00      0.00       982\n           5       0.87      0.84      0.86       892\n           6       0.92      0.93      0.93       958\n           7       0.91      0.94      0.92      1028\n           8       0.86      0.89      0.88       974\n           9       0.50      0.92      0.65      1009\n\n    accuracy                           0.83     10000\n   macro avg       0.78      0.83      0.80     10000\nweighted avg       0.78      0.83      0.80     10000\n\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(testLabels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 960    0    1    1    0    6    6    3    2    1]\n [   0 1119    2    4    0    1    3    1    5    0]\n [  14    9  931   15    0    5   11   17   24    6]\n [   1    1   24  919    0   20    0   18   25    2]\n [   2    4   10    0    0    3   42   28   25  868]\n [  13    2   12   55    0  749    7    5   41    8]\n [  26    3   11    0    0   17  895    0    4    2]\n [   2    9   24    3    0    1    0  968    0   21]\n [   4   14   14   17    0   36    8    6  869    6]\n [   9    5    0   10    0   19    4   19   13  930]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(testLabels, predictions))"
   ]
  },
  {
   "source": [
    "# Pytorch Dropout Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNet, self).__init__()\n",
    "        self.l1 = nn.Linear(28 * 28, 64)\n",
    "        self.l2 = nn.Linear(64, 64)\n",
    "        self.l3 = nn.Linear(64, 32)\n",
    "        self.do = nn.Dropout(0.1)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        l1 = self.relu(self.l1(x))\n",
    "        l2 = self.sigmoid(self.l2(l1))\n",
    "        do = self.do(l2 + l1)\n",
    "        l3 = self.softmax(self.l3(do))\n",
    "        return l3\n",
    "    \n",
    "\n",
    "d_model = DNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = DNet()\n",
    "d_loss_func = nn.CrossEntropyLoss()\n",
    "d_params = d_model.parameters()\n",
    "d_opt = optim.Adam(d_params, lr=0.001)\n",
    "d_epochs = 750\n",
    "d_iters = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss: 2.60,training accuracy: 0.94\n",
      "Epoch 388, training loss: 2.79,training accuracy: 0.75\n",
      "Epoch 389, training loss: 2.65,training accuracy: 0.91\n",
      "Epoch 390, training loss: 2.78,training accuracy: 0.75\n",
      "Epoch 391, training loss: 2.67,training accuracy: 0.88\n",
      "Epoch 392, training loss: 2.67,training accuracy: 0.89\n",
      "Epoch 393, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 394, training loss: 2.76,training accuracy: 0.77\n",
      "Epoch 395, training loss: 2.69,training accuracy: 0.88\n",
      "Epoch 396, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 397, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 398, training loss: 2.75,training accuracy: 0.78\n",
      "Epoch 399, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 400, training loss: 2.82,training accuracy: 0.70\n",
      "Epoch 401, training loss: 2.70,training accuracy: 0.86\n",
      "Epoch 402, training loss: 2.71,training accuracy: 0.84\n",
      "Epoch 403, training loss: 2.74,training accuracy: 0.81\n",
      "Epoch 404, training loss: 2.74,training accuracy: 0.80\n",
      "Epoch 405, training loss: 2.71,training accuracy: 0.86\n",
      "Epoch 406, training loss: 2.61,training accuracy: 0.94\n",
      "Epoch 407, training loss: 2.64,training accuracy: 0.91\n",
      "Epoch 408, training loss: 2.77,training accuracy: 0.75\n",
      "Epoch 409, training loss: 2.79,training accuracy: 0.73\n",
      "Epoch 410, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 411, training loss: 2.77,training accuracy: 0.75\n",
      "Epoch 412, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 413, training loss: 2.65,training accuracy: 0.89\n",
      "Epoch 414, training loss: 2.71,training accuracy: 0.83\n",
      "Epoch 415, training loss: 2.71,training accuracy: 0.83\n",
      "Epoch 416, training loss: 2.76,training accuracy: 0.78\n",
      "Epoch 417, training loss: 2.67,training accuracy: 0.88\n",
      "Epoch 418, training loss: 2.74,training accuracy: 0.80\n",
      "Epoch 419, training loss: 2.71,training accuracy: 0.80\n",
      "Epoch 420, training loss: 2.73,training accuracy: 0.78\n",
      "Epoch 421, training loss: 2.70,training accuracy: 0.84\n",
      "Epoch 422, training loss: 2.72,training accuracy: 0.83\n",
      "Epoch 423, training loss: 2.65,training accuracy: 0.89\n",
      "Epoch 424, training loss: 2.69,training accuracy: 0.86\n",
      "Epoch 425, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 426, training loss: 2.76,training accuracy: 0.78\n",
      "Epoch 427, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 428, training loss: 2.68,training accuracy: 0.88\n",
      "Epoch 429, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 430, training loss: 2.76,training accuracy: 0.78\n",
      "Epoch 431, training loss: 2.66,training accuracy: 0.89\n",
      "Epoch 432, training loss: 2.71,training accuracy: 0.84\n",
      "Epoch 433, training loss: 2.65,training accuracy: 0.86\n",
      "Epoch 434, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 435, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 436, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 437, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 438, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 439, training loss: 2.69,training accuracy: 0.88\n",
      "Epoch 440, training loss: 2.71,training accuracy: 0.83\n",
      "Epoch 441, training loss: 2.68,training accuracy: 0.88\n",
      "Epoch 442, training loss: 2.71,training accuracy: 0.83\n",
      "Epoch 443, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 444, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 445, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 446, training loss: 2.74,training accuracy: 0.80\n",
      "Epoch 447, training loss: 2.76,training accuracy: 0.77\n",
      "Epoch 448, training loss: 2.76,training accuracy: 0.75\n",
      "Epoch 449, training loss: 2.80,training accuracy: 0.72\n",
      "Epoch 450, training loss: 2.70,training accuracy: 0.86\n",
      "Epoch 451, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 452, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 453, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 454, training loss: 2.66,training accuracy: 0.86\n",
      "Epoch 455, training loss: 2.83,training accuracy: 0.69\n",
      "Epoch 456, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 457, training loss: 2.64,training accuracy: 0.89\n",
      "Epoch 458, training loss: 2.62,training accuracy: 0.91\n",
      "Epoch 459, training loss: 2.64,training accuracy: 0.89\n",
      "Epoch 460, training loss: 2.71,training accuracy: 0.83\n",
      "Epoch 461, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 462, training loss: 2.79,training accuracy: 0.73\n",
      "Epoch 463, training loss: 2.73,training accuracy: 0.83\n",
      "Epoch 464, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 465, training loss: 2.73,training accuracy: 0.81\n",
      "Epoch 466, training loss: 2.69,training accuracy: 0.88\n",
      "Epoch 467, training loss: 2.72,training accuracy: 0.83\n",
      "Epoch 468, training loss: 2.72,training accuracy: 0.81\n",
      "Epoch 469, training loss: 2.66,training accuracy: 0.86\n",
      "Epoch 470, training loss: 2.76,training accuracy: 0.80\n",
      "Epoch 471, training loss: 2.74,training accuracy: 0.80\n",
      "Epoch 472, training loss: 2.76,training accuracy: 0.78\n",
      "Epoch 473, training loss: 2.69,training accuracy: 0.88\n",
      "Epoch 474, training loss: 2.75,training accuracy: 0.78\n",
      "Epoch 475, training loss: 2.72,training accuracy: 0.83\n",
      "Epoch 476, training loss: 2.78,training accuracy: 0.77\n",
      "Epoch 477, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 478, training loss: 2.74,training accuracy: 0.77\n",
      "Epoch 479, training loss: 2.74,training accuracy: 0.80\n",
      "Epoch 480, training loss: 2.77,training accuracy: 0.77\n",
      "Epoch 481, training loss: 2.75,training accuracy: 0.83\n",
      "Epoch 482, training loss: 2.81,training accuracy: 0.70\n",
      "Epoch 483, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 484, training loss: 2.75,training accuracy: 0.78\n",
      "Epoch 485, training loss: 2.76,training accuracy: 0.77\n",
      "Epoch 486, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 487, training loss: 2.64,training accuracy: 0.88\n",
      "Epoch 488, training loss: 2.76,training accuracy: 0.78\n",
      "Epoch 489, training loss: 2.73,training accuracy: 0.81\n",
      "Epoch 490, training loss: 2.75,training accuracy: 0.78\n",
      "Epoch 491, training loss: 2.72,training accuracy: 0.81\n",
      "Epoch 492, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 493, training loss: 2.70,training accuracy: 0.84\n",
      "Epoch 494, training loss: 2.72,training accuracy: 0.81\n",
      "Epoch 495, training loss: 2.64,training accuracy: 0.92\n",
      "Epoch 496, training loss: 2.75,training accuracy: 0.80\n",
      "Epoch 497, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 498, training loss: 2.73,training accuracy: 0.81\n",
      "Epoch 499, training loss: 2.81,training accuracy: 0.72\n",
      "Epoch 500, training loss: 2.72,training accuracy: 0.83\n",
      "Epoch 501, training loss: 2.72,training accuracy: 0.81\n",
      "Epoch 502, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 503, training loss: 2.72,training accuracy: 0.83\n",
      "Epoch 504, training loss: 2.73,training accuracy: 0.83\n",
      "Epoch 505, training loss: 2.70,training accuracy: 0.84\n",
      "Epoch 506, training loss: 2.67,training accuracy: 0.84\n",
      "Epoch 507, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 508, training loss: 2.61,training accuracy: 0.92\n",
      "Epoch 509, training loss: 2.65,training accuracy: 0.89\n",
      "Epoch 510, training loss: 2.77,training accuracy: 0.75\n",
      "Epoch 511, training loss: 2.77,training accuracy: 0.78\n",
      "Epoch 512, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 513, training loss: 2.63,training accuracy: 0.91\n",
      "Epoch 514, training loss: 2.64,training accuracy: 0.88\n",
      "Epoch 515, training loss: 2.80,training accuracy: 0.72\n",
      "Epoch 516, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 517, training loss: 2.75,training accuracy: 0.80\n",
      "Epoch 518, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 519, training loss: 2.73,training accuracy: 0.81\n",
      "Epoch 520, training loss: 2.68,training accuracy: 0.88\n",
      "Epoch 521, training loss: 2.66,training accuracy: 0.91\n",
      "Epoch 522, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 523, training loss: 2.74,training accuracy: 0.81\n",
      "Epoch 524, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 525, training loss: 2.75,training accuracy: 0.78\n",
      "Epoch 526, training loss: 2.73,training accuracy: 0.78\n",
      "Epoch 527, training loss: 2.77,training accuracy: 0.78\n",
      "Epoch 528, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 529, training loss: 2.78,training accuracy: 0.75\n",
      "Epoch 530, training loss: 2.76,training accuracy: 0.78\n",
      "Epoch 531, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 532, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 533, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 534, training loss: 2.70,training accuracy: 0.81\n",
      "Epoch 535, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 536, training loss: 2.74,training accuracy: 0.77\n",
      "Epoch 537, training loss: 2.70,training accuracy: 0.84\n",
      "Epoch 538, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 539, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 540, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 541, training loss: 2.73,training accuracy: 0.83\n",
      "Epoch 542, training loss: 2.67,training accuracy: 0.89\n",
      "Epoch 543, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 544, training loss: 2.72,training accuracy: 0.83\n",
      "Epoch 545, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 546, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 547, training loss: 2.64,training accuracy: 0.92\n",
      "Epoch 548, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 549, training loss: 2.68,training accuracy: 0.83\n",
      "Epoch 550, training loss: 2.74,training accuracy: 0.78\n",
      "Epoch 551, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 552, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 553, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 554, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 555, training loss: 2.67,training accuracy: 0.84\n",
      "Epoch 556, training loss: 2.67,training accuracy: 0.84\n",
      "Epoch 557, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 558, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 559, training loss: 2.75,training accuracy: 0.78\n",
      "Epoch 560, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 561, training loss: 2.66,training accuracy: 0.89\n",
      "Epoch 562, training loss: 2.64,training accuracy: 0.91\n",
      "Epoch 563, training loss: 2.70,training accuracy: 0.81\n",
      "Epoch 564, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 565, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 566, training loss: 2.75,training accuracy: 0.78\n",
      "Epoch 567, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 568, training loss: 2.76,training accuracy: 0.78\n",
      "Epoch 569, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 570, training loss: 2.74,training accuracy: 0.78\n",
      "Epoch 571, training loss: 2.64,training accuracy: 0.89\n",
      "Epoch 572, training loss: 2.73,training accuracy: 0.80\n",
      "Epoch 573, training loss: 2.64,training accuracy: 0.91\n",
      "Epoch 574, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 575, training loss: 2.77,training accuracy: 0.75\n",
      "Epoch 576, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 577, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 578, training loss: 2.77,training accuracy: 0.77\n",
      "Epoch 579, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 580, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 581, training loss: 2.62,training accuracy: 0.91\n",
      "Epoch 582, training loss: 2.74,training accuracy: 0.78\n",
      "Epoch 583, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 584, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 585, training loss: 2.70,training accuracy: 0.83\n",
      "Epoch 586, training loss: 2.73,training accuracy: 0.81\n",
      "Epoch 587, training loss: 2.77,training accuracy: 0.77\n",
      "Epoch 588, training loss: 2.67,training accuracy: 0.84\n",
      "Epoch 589, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 590, training loss: 2.75,training accuracy: 0.80\n",
      "Epoch 591, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 592, training loss: 2.75,training accuracy: 0.77\n",
      "Epoch 593, training loss: 2.76,training accuracy: 0.77\n",
      "Epoch 594, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 595, training loss: 2.76,training accuracy: 0.78\n",
      "Epoch 596, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 597, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 598, training loss: 2.68,training accuracy: 0.88\n",
      "Epoch 599, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 600, training loss: 2.75,training accuracy: 0.78\n",
      "Epoch 601, training loss: 2.77,training accuracy: 0.77\n",
      "Epoch 602, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 603, training loss: 2.76,training accuracy: 0.77\n",
      "Epoch 604, training loss: 2.71,training accuracy: 0.83\n",
      "Epoch 605, training loss: 2.66,training accuracy: 0.89\n",
      "Epoch 606, training loss: 2.63,training accuracy: 0.89\n",
      "Epoch 607, training loss: 2.69,training accuracy: 0.86\n",
      "Epoch 608, training loss: 2.68,training accuracy: 0.83\n",
      "Epoch 609, training loss: 2.75,training accuracy: 0.78\n",
      "Epoch 610, training loss: 2.77,training accuracy: 0.78\n",
      "Epoch 611, training loss: 2.73,training accuracy: 0.81\n",
      "Epoch 612, training loss: 2.73,training accuracy: 0.78\n",
      "Epoch 613, training loss: 2.62,training accuracy: 0.91\n",
      "Epoch 614, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 615, training loss: 2.67,training accuracy: 0.88\n",
      "Epoch 616, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 617, training loss: 2.69,training accuracy: 0.81\n",
      "Epoch 618, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 619, training loss: 2.69,training accuracy: 0.86\n",
      "Epoch 620, training loss: 2.69,training accuracy: 0.88\n",
      "Epoch 621, training loss: 2.69,training accuracy: 0.86\n",
      "Epoch 622, training loss: 2.64,training accuracy: 0.91\n",
      "Epoch 623, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 624, training loss: 2.70,training accuracy: 0.84\n",
      "Epoch 625, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 626, training loss: 2.63,training accuracy: 0.94\n",
      "Epoch 627, training loss: 2.79,training accuracy: 0.73\n",
      "Epoch 628, training loss: 2.69,training accuracy: 0.86\n",
      "Epoch 629, training loss: 2.77,training accuracy: 0.77\n",
      "Epoch 630, training loss: 2.75,training accuracy: 0.81\n",
      "Epoch 631, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 632, training loss: 2.74,training accuracy: 0.78\n",
      "Epoch 633, training loss: 2.74,training accuracy: 0.81\n",
      "Epoch 634, training loss: 2.71,training accuracy: 0.84\n",
      "Epoch 635, training loss: 2.71,training accuracy: 0.80\n",
      "Epoch 636, training loss: 2.63,training accuracy: 0.91\n",
      "Epoch 637, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 638, training loss: 2.79,training accuracy: 0.72\n",
      "Epoch 639, training loss: 2.67,training accuracy: 0.84\n",
      "Epoch 640, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 641, training loss: 2.73,training accuracy: 0.80\n",
      "Epoch 642, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 643, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 644, training loss: 2.77,training accuracy: 0.78\n",
      "Epoch 645, training loss: 2.67,training accuracy: 0.89\n",
      "Epoch 646, training loss: 2.72,training accuracy: 0.80\n",
      "Epoch 647, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 648, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 649, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 650, training loss: 2.73,training accuracy: 0.80\n",
      "Epoch 651, training loss: 2.72,training accuracy: 0.81\n",
      "Epoch 652, training loss: 2.70,training accuracy: 0.84\n",
      "Epoch 653, training loss: 2.69,training accuracy: 0.86\n",
      "Epoch 654, training loss: 2.66,training accuracy: 0.86\n",
      "Epoch 655, training loss: 2.64,training accuracy: 0.88\n",
      "Epoch 656, training loss: 2.73,training accuracy: 0.81\n",
      "Epoch 657, training loss: 2.68,training accuracy: 0.84\n",
      "Epoch 658, training loss: 2.65,training accuracy: 0.89\n",
      "Epoch 659, training loss: 2.67,training accuracy: 0.88\n",
      "Epoch 660, training loss: 2.73,training accuracy: 0.80\n",
      "Epoch 661, training loss: 2.84,training accuracy: 0.69\n",
      "Epoch 662, training loss: 2.63,training accuracy: 0.91\n",
      "Epoch 663, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 664, training loss: 2.60,training accuracy: 0.95\n",
      "Epoch 665, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 666, training loss: 2.75,training accuracy: 0.78\n",
      "Epoch 667, training loss: 2.67,training accuracy: 0.84\n",
      "Epoch 668, training loss: 2.65,training accuracy: 0.89\n",
      "Epoch 669, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 670, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 671, training loss: 2.60,training accuracy: 0.92\n",
      "Epoch 672, training loss: 2.64,training accuracy: 0.89\n",
      "Epoch 673, training loss: 2.63,training accuracy: 0.91\n",
      "Epoch 674, training loss: 2.61,training accuracy: 0.92\n",
      "Epoch 675, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 676, training loss: 2.76,training accuracy: 0.77\n",
      "Epoch 677, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 678, training loss: 2.63,training accuracy: 0.91\n",
      "Epoch 679, training loss: 2.63,training accuracy: 0.89\n",
      "Epoch 680, training loss: 2.62,training accuracy: 0.92\n",
      "Epoch 681, training loss: 2.65,training accuracy: 0.88\n",
      "Epoch 682, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 683, training loss: 2.65,training accuracy: 0.91\n",
      "Epoch 684, training loss: 2.65,training accuracy: 0.89\n",
      "Epoch 685, training loss: 2.61,training accuracy: 0.94\n",
      "Epoch 686, training loss: 2.66,training accuracy: 0.91\n",
      "Epoch 687, training loss: 2.68,training accuracy: 0.83\n",
      "Epoch 688, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 689, training loss: 2.66,training accuracy: 0.89\n",
      "Epoch 690, training loss: 2.61,training accuracy: 0.94\n",
      "Epoch 691, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 692, training loss: 2.61,training accuracy: 0.94\n",
      "Epoch 693, training loss: 2.66,training accuracy: 0.89\n",
      "Epoch 694, training loss: 2.68,training accuracy: 0.86\n",
      "Epoch 695, training loss: 2.71,training accuracy: 0.81\n",
      "Epoch 696, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 697, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 698, training loss: 2.68,training accuracy: 0.88\n",
      "Epoch 699, training loss: 2.73,training accuracy: 0.83\n",
      "Epoch 700, training loss: 2.66,training accuracy: 0.86\n",
      "Epoch 701, training loss: 2.60,training accuracy: 0.95\n",
      "Epoch 702, training loss: 2.60,training accuracy: 0.94\n",
      "Epoch 703, training loss: 2.64,training accuracy: 0.89\n",
      "Epoch 704, training loss: 2.64,training accuracy: 0.92\n",
      "Epoch 705, training loss: 2.65,training accuracy: 0.89\n",
      "Epoch 706, training loss: 2.74,training accuracy: 0.78\n",
      "Epoch 707, training loss: 2.61,training accuracy: 0.92\n",
      "Epoch 708, training loss: 2.62,training accuracy: 0.94\n",
      "Epoch 709, training loss: 2.64,training accuracy: 0.91\n",
      "Epoch 710, training loss: 2.65,training accuracy: 0.91\n",
      "Epoch 711, training loss: 2.71,training accuracy: 0.83\n",
      "Epoch 712, training loss: 2.61,training accuracy: 0.95\n",
      "Epoch 713, training loss: 2.60,training accuracy: 0.94\n",
      "Epoch 714, training loss: 2.61,training accuracy: 0.92\n",
      "Epoch 715, training loss: 2.59,training accuracy: 0.95\n",
      "Epoch 716, training loss: 2.61,training accuracy: 0.95\n",
      "Epoch 717, training loss: 2.66,training accuracy: 0.86\n",
      "Epoch 718, training loss: 2.63,training accuracy: 0.89\n",
      "Epoch 719, training loss: 2.63,training accuracy: 0.91\n",
      "Epoch 720, training loss: 2.69,training accuracy: 0.83\n",
      "Epoch 721, training loss: 2.57,training accuracy: 0.98\n",
      "Epoch 722, training loss: 2.67,training accuracy: 0.86\n",
      "Epoch 723, training loss: 2.61,training accuracy: 0.94\n",
      "Epoch 724, training loss: 2.63,training accuracy: 0.89\n",
      "Epoch 725, training loss: 2.64,training accuracy: 0.92\n",
      "Epoch 726, training loss: 2.66,training accuracy: 0.91\n",
      "Epoch 727, training loss: 2.62,training accuracy: 0.94\n",
      "Epoch 728, training loss: 2.57,training accuracy: 0.98\n",
      "Epoch 729, training loss: 2.71,training accuracy: 0.86\n",
      "Epoch 730, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 731, training loss: 2.69,training accuracy: 0.84\n",
      "Epoch 732, training loss: 2.63,training accuracy: 0.91\n",
      "Epoch 733, training loss: 2.62,training accuracy: 0.94\n",
      "Epoch 734, training loss: 2.62,training accuracy: 0.92\n",
      "Epoch 735, training loss: 2.62,training accuracy: 0.94\n",
      "Epoch 736, training loss: 2.62,training accuracy: 0.92\n",
      "Epoch 737, training loss: 2.66,training accuracy: 0.88\n",
      "Epoch 738, training loss: 2.65,training accuracy: 0.89\n",
      "Epoch 739, training loss: 2.62,training accuracy: 0.92\n",
      "Epoch 740, training loss: 2.63,training accuracy: 0.92\n",
      "Epoch 741, training loss: 2.63,training accuracy: 0.91\n",
      "Epoch 742, training loss: 2.61,training accuracy: 0.95\n",
      "Epoch 743, training loss: 2.67,training accuracy: 0.88\n",
      "Epoch 744, training loss: 2.59,training accuracy: 0.97\n",
      "Epoch 745, training loss: 2.70,training accuracy: 0.81\n",
      "Epoch 746, training loss: 2.61,training accuracy: 0.95\n",
      "Epoch 747, training loss: 2.64,training accuracy: 0.89\n",
      "Epoch 748, training loss: 2.63,training accuracy: 0.92\n",
      "Epoch 749, training loss: 2.68,training accuracy: 0.88\n",
      "Epoch 750, training loss: 2.60,training accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "for d_epoch in range(d_epochs):\n",
    "    d_losses = list()\n",
    "    d_accuracies = list()\n",
    "    d_model.train()\n",
    "    for i, (d_images, d_labels) in enumerate(train_loader):\n",
    "        d_images = Variable(d_images)\n",
    "        d_labels = Variable(d_labels)\n",
    "\n",
    "        d_output = d_model(d_images)\n",
    "\n",
    "        d_model.zero_grad()\n",
    "        d_loss = d_loss_func(d_output, d_labels)\n",
    "        d_loss.backward() \n",
    "\n",
    "        d_opt.step() \n",
    "\n",
    "        d_losses.append(d_loss.item())\n",
    "        d_accuracies.append(d_labels.eq(d_output.detach().argmax(dim=1)).float().mean())\n",
    "\n",
    "        print(f'Epoch {d_epoch + 1 }', end= ', ' )\n",
    "        print(f'training loss: {torch.tensor(d_losses).mean():.2f}', end=',')\n",
    "        print(f'training accuracy: {torch.tensor(d_accuracies).mean():.2f}')\n",
    "\n",
    "        d_iters += 1\n",
    "        break"
   ]
  },
  {
   "source": [
    "# Convolutional Neural Network Predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    d_predictions, d_testLabels = get_eval(d_model, test_loader)\n",
    "    \n",
    "d_testLabels =  to_categorical(d_testLabels)\n",
    "d_testLabels = np.argmax(d_testLabels, axis=1)\n",
    "d_predictions = np.argmax(d_predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction:  [0 2 1 8 5 7 0 9 3 7 9 6 5 1 4 2 7 2 4 1]\nActual:  [0 2 1 8 5 7 0 9 3 7 9 6 5 1 4 2 7 2 4 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction: \", d_predictions[:20])\n",
    "print(\"Actual: \",d_testLabels[:20])"
   ]
  },
  {
   "source": [
    "# Classification Report and Confusion Matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.93      0.98      0.95       980\n           1       0.96      0.97      0.97      1135\n           2       0.94      0.87      0.90      1032\n           3       0.93      0.87      0.90      1010\n           4       0.88      0.93      0.91       982\n           5       0.90      0.83      0.86       892\n           6       0.94      0.93      0.94       958\n           7       0.93      0.90      0.92      1028\n           8       0.75      0.84      0.79       974\n           9       0.88      0.89      0.89      1009\n\n    accuracy                           0.90     10000\n   macro avg       0.90      0.90      0.90     10000\nweighted avg       0.91      0.90      0.90     10000\n\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(d_testLabels, d_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 958    0    0    1    1    5    7    2    6    0]\n [   0 1104    5    1    1    3    5    1   15    0]\n [  16    2  895   12   17    0    8   18   61    3]\n [   3    0   17  877    1   34    2   12   55    9]\n [   1    3    3    0  914    1    6    2   14   38]\n [  11    3    4   25   15  740   12    9   67    6]\n [  15    3    0    0   16   16  892    0   16    0]\n [   3   11   15    3    9    0    0  930   19   38]\n [  12   18    8   18   22   23   14   12  821   26]\n [  11    9    1    2   39    3    1   13   27  903]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(d_testLabels, d_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}